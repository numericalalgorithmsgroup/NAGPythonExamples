{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying web pages using non-negative matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15 URLs that need to be automatically classified\n",
    "\n",
    "The file `brexit_trump_urls.txt` contains a set of 15 links to news articles.  Some of them are about Donald Trump and some of them are about Brexit. A human could easily work out which are which by reading them but here we will demonstrate how to do this automatically using Non-negative matrix factorization via the NAG Library for Python's **real_nmf** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.bbc.co.uk/news/uk-politics-47031312\n",
      "https://www.bbc.co.uk/news/world-us-canada-47477727\n",
      "https://www.bbc.co.uk/news/uk-scotland-north-east-orkney-shetland-47642076?intlink_from_url=&link_location=live-reporting-story\n",
      "https://www.bbc.co.uk/news/uk-wales-politics-47651013?intlink_from_url=https://www.bbc.co.uk/news/topics/cwlw3xz0lvvt/brexit&link_location=live-reporting-story\n",
      "https://www.bbc.co.uk/news/av/world-us-canada-47646183/president-trump-shows-map-of-is-defeat?intlink_from_url=&link_location=live-reporting-map\n",
      "https://www.bbc.co.uk/news/uk-politics-46393399\n",
      "https://www.bbc.co.uk/news/business-47644268?intlink_from_url=&link_location=live-reporting-story\n",
      "https://www.bbc.co.uk/news/world-us-canada-47633940\n",
      "https://www.bbc.co.uk/news/uk-politics-47627744\n",
      "https://www.bbc.co.uk/news/uk-politics-parliaments-47653160\n",
      "https://www.bbc.co.uk/news/world-us-canada-47642335\n",
      "https://www.bbc.co.uk/news/uk-politics-47660019\n",
      "https://www.bbc.co.uk/news/world-middle-east-47657843\n",
      "https://www.bbc.co.uk/news/uk-politics-47659410\n",
      "https://www.bbc.co.uk/news/uk-politics-47652071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the URLs in the file brexit_trump_urls.txt\n",
    "with open('brexit_trump_urls.txt') as f:\n",
    "    read_data = f.read()\n",
    "print(read_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the websites "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to download all of the articles and parse them into sets of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading urls\n",
      "Parsing webpages\n",
      "['Brexit delay: How is Article 50 extended? - BBC News']\n",
      "['Kirstjen Nielsen: Walking a tightrope working for Trump - BBC News']\n",
      "['Trump homes plan at Menie being recommended for approval - BBC News']\n",
      "['Theresa May at her worst during Brexit speech - Mark Drakeford - BBC News']\n",
      "[\"President Trump shows map of 'IS defeat' - BBC News\"]\n",
      "['Brexit: What happens now? - BBC News']\n",
      "['Trump spooks markets with China trade tariffs warning - BBC News']\n",
      "['A tale of two Trumps: Jair Bolsonaro goes to Washington - BBC News']\n",
      "['Brexit: Theresa May to formally ask for delay - BBC News']\n",
      "['Corbyn calls for compromise to avoid no-deal Brexit - BBC News']\n",
      "[\"Trump: I didn't get a thank you for McCain funeral - BBC News\"]\n",
      "['Brexit: EU leaders agree Article 50 delay plan - BBC News']\n",
      "['Trump: Time to recognise Golan Heights as Israeli territory - BBC News']\n",
      "['Brexit: MPs urged not to travel home alone as tensions rise - BBC News']\n",
      "[\"'Cancel Brexit' petition passes 2m signatures on Parliament site - BBC News\"]\n"
     ]
    }
   ],
   "source": [
    "from naginterfaces.library.matop import real_nmf\n",
    "from collections import Counter\n",
    "import string\n",
    "import urllib.request\n",
    "import re\n",
    "from scipy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "print(\"Reading urls\")\n",
    "with open('brexit_trump_urls.txt', 'r') as f:\n",
    "    links = f.readlines()\n",
    "\n",
    "n_links = len(links)\n",
    "\n",
    "dicts = []\n",
    "titles = []\n",
    "words = set()\n",
    "trans = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "\n",
    "print(\"Parsing webpages\")\n",
    "for link in links:\n",
    "    f1 = urllib.request.urlopen(link)\n",
    "    pagewords = []\n",
    "    paras = re.findall(r'<p>(.*?)</p>', f1.read().decode().lower())\n",
    "    f2 = urllib.request.urlopen(link)\n",
    "    title = re.findall(r'<title>(.*?)</title>', f2.read().decode())\n",
    "    print(title)\n",
    "    titles.append(title)\n",
    "    for para in paras:\n",
    "        pagewords += para.translate(trans).split()\n",
    "\n",
    "    c = Counter(pagewords)\n",
    "    dicts.append(c)\n",
    "    words = words | set(list(c.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `dicts` contains 15 dictionaries where each one corresponds to word frequencies for each URL.  \n",
    "Here are the most common words in the first URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 32),\n",
       " ('the', 29),\n",
       " ('to', 16),\n",
       " ('uk', 15),\n",
       " ('of', 13),\n",
       " ('in', 10),\n",
       " ('that', 9),\n",
       " ('it', 9),\n",
       " ('on', 9),\n",
       " ('eu', 8),\n",
       " ('would', 8),\n",
       " ('extension', 7),\n",
       " ('href', 7),\n",
       " ('class', 7),\n",
       " ('story', 7),\n",
       " ('body', 7),\n",
       " ('link', 7),\n",
       " ('an', 6),\n",
       " ('article', 6),\n",
       " ('50', 6)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts[0].most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `words` contains the list of all the words we've seen from all of the URLs in alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2186"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up the word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list of 2279 words contains many common words such as 'the', 'that' and 'with' which we want to ignore.  \n",
    "These unwanted words are commonly referred to as [stopwords](https://en.wikipedia.org/wiki/Stop_words). \n",
    "The explicit list of stopwords we are going to use in this analysis are defined in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['then', 'that', 'have', 'with', 'from', 'they', 'here', 'there', 'their', 'would', 'what', 'which', 'about', 'know',\n",
    "        'just', 'time', 'like', 'make', 'your', 'year', 'some', 'good', 'into', 'people', 'them', 'other', 'than', 'look', \n",
    "        'only', 'over', 'think', 'also', 'back', 'after', 'work', 'first', 'well', 'even', 'want', 'because', 'these', \n",
    "        'most', 'leave', 'seem', 'come', 'little', 'last', 'long', 'great', 'high', 'small', 'large', 'next', 'early',\n",
    "        'same', 'this', 'away', 'down', 'look', 'make', 'three', 'came', 'four', 'please', 'pretty', 'soon', 'under', \n",
    "        'went', 'white', 'black', 'give', 'giving', 'given', 'gave', 'knowing', 'knew', 'once', 'round', 'stop', 'take', \n",
    "        'taken', 'took', 'thank', 'thanks', 'walk', 'walked', 'always', 'around', 'been', 'before', 'best', 'worst', 'find',\n",
    "        'found', 'goes', 'pull', 'read', 'right', 'wrong', 'tell', 'telling', 'told', 'upon', 'wish', 'write', 'written', \n",
    "        'better', 'carry', 'carried', 'full', 'hold', 'keep', 'kept', 'longer', 'longest', 'shall', 'will', 'begin',\n",
    "        'beginning', 'together', 'today', 'yesterday', 'children', 'ground', 'hold', 'holding', 'morning', 'afternoon',\n",
    "        'never', 'myself', 'table', 'water', 'wind', 'window', 'ring', 'rung', 'except', 'where', 'while', 'woman', \n",
    "        'whilst', 'were', 'until', 'thing', 'things', 'theirs', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', \n",
    "        'saturday', 'sunday', 'should', 'shown', 'shut', 'another', 'being', 'does', 'doing', 'make', 'makes', 'more', \n",
    "        'name', 'names', 'named', 'through', 'years', 'used', 'said', 'saying', 'says', 'seen', 'sees', 'seeing', \n",
    "        'using', 'uses', 'known', 'left', 'send', 'sent', 'choose', 'choice', 'choosing', 'going', 'gets', 'below', \n",
    "        'less', 'least', 'might', 'href', 'link', 'https', 'a939', 'abbe', 'ac30f6e24b3f', 'ababa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove them from our word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in words if not word in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also remove any words with fewer than 4 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in words if not len(word)<4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1795"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now remove any words starting with a digit or a pound sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in words if not word.startswith('£')]\n",
    "words = [word for word in words if not word.startswith('1')]\n",
    "words = [word for word in words if not word.startswith('2')]\n",
    "words = [word for word in words if not word.startswith('3')]\n",
    "words = [word for word in words if not word.startswith('4')]\n",
    "words = [word for word in words if not word.startswith('5')]\n",
    "words = [word for word in words if not word.startswith('6')]\n",
    "words = [word for word in words if not word.startswith('7')]\n",
    "words = [word for word in words if not word.startswith('8')]\n",
    "words = [word for word in words if not word.startswith('9')]\n",
    "words = [word for word in words if not word.startswith('0')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1753 distinct words will be used to form the data matrix\n"
     ]
    }
   ],
   "source": [
    "print(\"{0} distinct words will be used to form the data matrix\".format(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the data matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to form a data matrix using these words where row $(i)$ refers to a word and each column $(j)$ refers to a URL.  The $(i,j)^{th}$ entry of this matrix will contain the frequency of occurence of the word $i$ in the URL $j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data matrix\n",
      "Final data matrix has size:(1753, 15)\n"
     ]
    }
   ],
   "source": [
    "# Now create a dict with the words as keys and tuples for the word counts in each webpage\n",
    "masterdict = {}\n",
    "\n",
    "print(\"Creating data matrix\")\n",
    "for word in words:\n",
    "    # Create list containing the word counts\n",
    "    freqs = [dic.get(word, 0) for dic in dicts]\n",
    "    masterdict[word] = freqs\n",
    "\n",
    "a = np.asfortranarray(np.array(list(masterdict.values())), dtype=np.float64)\n",
    "\n",
    "print(\"Final data matrix has size:{0}\".format(a.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data matrix is a set of observations of a number of variables. In our matrix of word counts above, the observations were individual web pages and the variables were the frequencies of the different words on those pages, but there are numerous other examples. For instance, the observations might be pixels in an image, with spectrometry data as the variables. Or observations might correspond to different people, with various test results as the variables. In this analysis we are going to assume that each column of our data matrix corresponds to an observation and each row corresponds to a variable.\n",
    "\n",
    "Data matrices often have the following properties.\n",
    "\n",
    "* Their entries are non-negative. This isn't always the case but it is true for many important applications.\n",
    "* They can be very large and may be sparse. We recently encountered a data matrix from seismic tomography which had size 87000×67000, but with only 0.23% of nonzero entries.\n",
    "\n",
    "Large matrices are cumbersome to deal with. So it is natural to ask whether we can encapsulate the data using a smaller matrix, especially if our data matrix contains many zeros. Various techniques exist to do this, for example principal components analysis, or linear discriminant analysis. The drawback of these methods is that they do not preserve the non-negativity of the original data matrix, making the results potentially difficult to interpret. This is where non-negative matrix factorization comes in.\n",
    "\n",
    "Non-negative matrix factorization (NMF) takes a non-negative data matrix S and attempts to factor it into the product of a tall, skinny matrix **W (known as the features matrix)** and a **short, wide matrix H (the coefficients matrix)**. \n",
    "\n",
    "Both W and H are non-negative. This is shown in the graphic below. **Note the presence of ≈ rather than =** since an exact NMF may not exist. In practice NMF algorithms iterate towards acceptable solutions, rather than obtaining the optimal solution. \n",
    "\n",
    "![Non negative Matrix Factorisation](nmf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now output the data matrix to the file `wordcounts.txt` for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing data matrix to file\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing data matrix to file\")\n",
    "f2 = open('wordcounts.txt', 'w')\n",
    "\n",
    "st = ' '*14\n",
    "for i in range(n_links):\n",
    "    tmp = 'link ' + str(i+1)\n",
    "    st += tmp.ljust(8, ' ')\n",
    "\n",
    "print(st, file=f2)\n",
    "\n",
    "for i, v in enumerate(words):\n",
    "    st = v.ljust(14,' ')\n",
    "    for j in range(n_links):\n",
    "        st += str(int(a[i,j])).center(8, ' ')\n",
    "    print(st, file=f2)\n",
    "\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the non-negative matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use Non-Negative matrix factorisation to compute the matrices $w$ and $h$ such that \n",
    "\n",
    "$$a = w@h$$\n",
    "\n",
    "where $@$ is the standard Python operator for matrix-matrix multiplication.\n",
    "\n",
    "As of Mark 27, The NAG library has two routines for computing non-negative matrix factorisation: **real_nmf** and **real_nmf_rcomm** and here we will use **real_nmf** with k=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-fb37620fc264>:8: NagAlgorithmicWarning: (NAG Python function naginterfaces.base.matop.real_nmf, code 7:7,99992)\n",
      "** The function has failed to converge after 500 iterations.\n",
      "** The factorization given by w and h may still be a good enough approximation\n",
      "** to be useful. Alternatively an improved factorization may be obtained by\n",
      "** increasing maxit or using different initial choices of w and h.\n",
      "  w, h = real_nmf(a, k=2, seed=seed, errtol=errtol, maxit=maxit)\n"
     ]
    }
   ],
   "source": [
    "m = a.shape[0]\n",
    "n = a.shape[1]\n",
    "k = 2\n",
    "errtol = 1e-6\n",
    "maxit = 500\n",
    "seed = 5842\n",
    "\n",
    "w, h = real_nmf(a, k=2, seed=seed, errtol=errtol, maxit=maxit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1753, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 15)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factorization is not exact but is hopefully close enough to be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm of residual:\n",
      "0.7313733530687229\n"
     ]
    }
   ],
   "source": [
    "resnorm = norm(a-w@h)/norm(a)\n",
    "print(\"norm of residual:\")\n",
    "print(resnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strength of NMF is that the preservation of non-negativity makes it easier to interpret the factors W and H. In general W tells us how the different variables can be grouped together into k features that in some way represent the data. The matrix H tells us how our original observations are built from these features, with the non-negativity ensuring this is done in a purely additive manner.\n",
    "\n",
    "The best way of understanding this is to go back to our original example. Recall that we had a 1834x15 matrix of word frequencies for our 15 web pages. We used the NAG library routine **real_nmf** to factorize it, choosing k=2. This resulted in a 1834×2 features matrix, W and a 2×15 coefficients matrix H. Let's discuss them in turn.\n",
    "\n",
    "Each column of W corresponds to a particular weighted grouping of the 1834 distinct words from the articles. The larger the entries in the column, the more important the corresponding word is deemed to be. Rather than displaying W in its entirety, We can look at the 10 largest entries in each column to see what the most important words were. The results are shown in below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The most important words in column 1 of w are:\n",
      "['deal', 'brexit', 'parliament', 'delay', 'body', 'story', 'class', 'minister', 'prime', 'vote']\n",
      "\n",
      "The most important words in column 2 of w are:\n",
      "['trump', 'president', 'women', 'nielsen', 'border', 'mccain', 'security', 'senator', 'secretary', 'administration']\n"
     ]
    }
   ],
   "source": [
    "for i in range(k):\n",
    "    tmp = sorted(words, key=lambda x, ind=i: w[words.index(x),ind], reverse=True)\n",
    "    st = \"\\nThe most important words in column \" + str(i+1) + \" of w are:\"\n",
    "    print(st)\n",
    "    print(tmp[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these lists, you'll hopefully agree that the first column corresponds to Brexit and the second column Trump. It seems that our non-negative matrix factorization has successfully detected the two categories of web page. Let's denote these using the numbers 0 and 1. Can we now use the NMF to accurately categorise the individual pages? To do this we need to look at the coefficients matrix H.\n",
    "\n",
    "We convert h to a pandas dataframe for display purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link1</th>\n",
       "      <th>link2</th>\n",
       "      <th>link3</th>\n",
       "      <th>link4</th>\n",
       "      <th>link5</th>\n",
       "      <th>link6</th>\n",
       "      <th>link7</th>\n",
       "      <th>link8</th>\n",
       "      <th>link9</th>\n",
       "      <th>link10</th>\n",
       "      <th>link11</th>\n",
       "      <th>link12</th>\n",
       "      <th>link13</th>\n",
       "      <th>link14</th>\n",
       "      <th>link15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.072</td>\n",
       "      <td>7.953e-16</td>\n",
       "      <td>2.419</td>\n",
       "      <td>19.137</td>\n",
       "      <td>7.953e-16</td>\n",
       "      <td>4.437</td>\n",
       "      <td>1.823</td>\n",
       "      <td>0.044</td>\n",
       "      <td>3.159e+01</td>\n",
       "      <td>7.113e+00</td>\n",
       "      <td>7.953e-16</td>\n",
       "      <td>15.560</td>\n",
       "      <td>4.042</td>\n",
       "      <td>1.429</td>\n",
       "      <td>2.800e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.545</td>\n",
       "      <td>3.538e+01</td>\n",
       "      <td>2.133</td>\n",
       "      <td>1.220</td>\n",
       "      <td>1.578e+00</td>\n",
       "      <td>0.140</td>\n",
       "      <td>2.411</td>\n",
       "      <td>11.886</td>\n",
       "      <td>7.621e-16</td>\n",
       "      <td>7.621e-16</td>\n",
       "      <td>2.282e+01</td>\n",
       "      <td>0.208</td>\n",
       "      <td>17.007</td>\n",
       "      <td>0.203</td>\n",
       "      <td>7.621e-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    link1      link2  link3   link4      link5  link6  link7   link8  \\\n",
       "0  12.072  7.953e-16  2.419  19.137  7.953e-16  4.437  1.823   0.044   \n",
       "1   1.545  3.538e+01  2.133   1.220  1.578e+00  0.140  2.411  11.886   \n",
       "\n",
       "       link9     link10     link11  link12  link13  link14     link15  \n",
       "0  3.159e+01  7.113e+00  7.953e-16  15.560   4.042   1.429  2.800e+01  \n",
       "1  7.621e-16  7.621e-16  2.282e+01   0.208  17.007   0.203  7.621e-16  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert h to pandas dataframe for display purposes\n",
    "import pandas as pd\n",
    "display_h = pd.DataFrame(\n",
    "    h,\n",
    "    columns=[\n",
    "        \"link1\",\"link2\",\"link3\",\"link4\",\"link5\",\"link6\",\"link7\",\"link8\",\"link9\",\n",
    "        \"link10\",\"link11\",\"link12\",\"link13\",\"link14\",\"link15\",\n",
    "    ]\n",
    ")\n",
    "pd.set_option('precision', 3)\n",
    "display_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This coefficients matrix is of size 2×15. The entries in each column show us how well that particular web page fits into our two categories. We assigned each page to a category by simply selecting the largest entry in the column. The results are below. The number next to each link shows how it was categorised by the NMF. We will let you judge for yourself whether the categorisations are correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article \"Brexit delay: How is Article 50 extended? - BBC News\"                         is in category 0 (Trump)\n",
      "Article \"Kirstjen Nielsen: Walking a tightrope working for Trump - BBC News\"           is in category 1 (Brexit)\n",
      "Article \"Trump homes plan at Menie being recommended for approval - BBC News\"          is in category 0 (Trump)\n",
      "Article \"Theresa May at her worst during Brexit speech - Mark Drakeford - BBC News\"    is in category 0 (Trump)\n",
      "Article \"President Trump shows map of 'IS defeat' - BBC News\"                          is in category 1 (Brexit)\n",
      "Article \"Brexit: What happens now? - BBC News\"                                         is in category 0 (Trump)\n",
      "Article \"Trump spooks markets with China trade tariffs warning - BBC News\"             is in category 1 (Brexit)\n",
      "Article \"A tale of two Trumps: Jair Bolsonaro goes to Washington - BBC News\"           is in category 1 (Brexit)\n",
      "Article \"Brexit: Theresa May to formally ask for delay - BBC News\"                     is in category 0 (Trump)\n",
      "Article \"Corbyn calls for compromise to avoid no-deal Brexit - BBC News\"               is in category 0 (Trump)\n",
      "Article \"Trump: I didn't get a thank you for McCain funeral - BBC News\"                is in category 1 (Brexit)\n",
      "Article \"Brexit: EU leaders agree Article 50 delay plan - BBC News\"                    is in category 0 (Trump)\n",
      "Article \"Trump: Time to recognise Golan Heights as Israeli territory - BBC News\"       is in category 1 (Brexit)\n",
      "Article \"Brexit: MPs urged not to travel home alone as tensions rise - BBC News\"       is in category 0 (Trump)\n",
      "Article \"'Cancel Brexit' petition passes 2m signatures on Parliament site - BBC News\"  is in category 0 (Trump)\n"
     ]
    }
   ],
   "source": [
    "for i, link in enumerate(links):\n",
    "    category = 0 if h[0,i] > h[1,i] else 1\n",
    "    title = '\"' + (titles[i][0]) + '\"'\n",
    "    st = 'Article ' + title.ljust(78,' ') + ' is in category ' + str(category) + [\" (Trump)\",\" (Brexit)\"][category]\n",
    "    print(st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Reading\n",
    "\n",
    "* [Presentation on Non-Negative Matrix Factorization](https://www.nag.com/market/non-negative-matrix-factorization.pdf) by the author of the NAG routines **real_nmf** and **real_nmf_rcomm**\n",
    "* [NAG Blog post on which this notebook is based](https://www.nag.com/content/classifying-web-pages-using-non-negative-matrix-factorization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
